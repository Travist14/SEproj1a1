# import asyncio
# import os
# from typing import List, Optional

# from fastapi import FastAPI, HTTPException
# from fastapi.middleware.cors import CORSMiddleware
# from pydantic import BaseModel, Field
# from vllm import LLM, SamplingParams

# DEFAULT_MODEL = os.getenv(
#     "VLLM_MODEL_NAME", "meta-llama/Meta-Llama-3.1-8B-Instruct"
# )
# DEFAULT_MAX_TOKENS = int(os.getenv("VLLM_MAX_TOKENS", 8192))
# ALLOW_ORIGINS = tuple(
#     origin.strip()
#     for origin in os.getenv("BACKEND_ALLOW_ORIGINS", "*").split(",")
#     if origin.strip()
# )

# app = FastAPI(
#     title="SEproj1a1 vLLM Backend",
#     version="1.0.0",
#     description="FastAPI server that wraps a local vLLM inference engine.",
# )

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=list(ALLOW_ORIGINS) or ["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )


# class GenerateRequest(BaseModel):
#     prompt: str = Field(..., min_length=1, description="User prompt to send to the model.")
#     max_tokens: int = Field(
#         DEFAULT_MAX_TOKENS,
#         gt=0,
#         le=4096,
#         description="Number of tokens to generate in the completion.",
#     )
#     temperature: float = Field(
#         0.7,
#         ge=0.0,
#         le=2.0,
#         description="Sampling temperature.",
#     )
#     top_p: float = Field(
#         0.95,
#         ge=0.0,
#         le=1.0,
#         description="Top-p nucleus sampling parameter.",
#     )
#     stop: Optional[List[str]] = Field(
#         default=None,
#         description="Optional list of stop strings.",
#     )


# class GenerateResponse(BaseModel):
#     text: str
#     model: str


# class HealthResponse(BaseModel):
#     status: str
#     model: str


# _llm: Optional[LLM] = None
# _load_lock = asyncio.Lock()


# async def _load_model() -> LLM:
#     """Ensure the global LLM instance is created only once."""
#     global _llm
#     if _llm is None:
#         async with _load_lock:
#             if _llm is None:
#                 tensor_parallel = os.getenv("VLLM_TENSOR_PARALLEL_SIZE")
#                 kwargs = {}
#                 if tensor_parallel:
#                     try:
#                         kwargs["tensor_parallel_size"] = int(tensor_parallel)
#                     except ValueError:
#                         raise RuntimeError(
#                             "VLLM_TENSOR_PARALLEL_SIZE must be an integer."
#                         )

#                 trust_remote_code = os.getenv("VLLM_TRUST_REMOTE_CODE", "false").lower()
#                 if trust_remote_code in {"true", "1", "yes"}:
#                     kwargs["trust_remote_code"] = True

#                 # Load the model off the event loop thread to avoid blocking uvicorn.
#                 _llm = await asyncio.to_thread(LLM, model=DEFAULT_MODEL, **kwargs)
#     return _llm


# @app.on_event("startup")
# async def startup_event() -> None:
#     await _load_model()


# @app.get("/health", response_model=HealthResponse)
# async def health_check() -> HealthResponse:
#     model_name = DEFAULT_MODEL if _llm is None else _llm.llm_engine.model_config.model
#     return HealthResponse(status="ok", model=model_name)


# @app.post("/generate", response_model=GenerateResponse)
# async def generate_text(request: GenerateRequest) -> GenerateResponse:
#     llm = await _load_model()
#     sampling_params = SamplingParams(
#         temperature=request.temperature,
#         top_p=request.top_p,
#         max_tokens=request.max_tokens,
#         stop=request.stop,
#     )

#     try:
#         outputs = await asyncio.to_thread(
#             llm.generate,
#             [request.prompt],
#             sampling_params=sampling_params,
#         )
#     except Exception as exc:  # pragma: no cover - surface runtime issues
#         raise HTTPException(status_code=500, detail=str(exc)) from exc

#     if not outputs or not outputs[0].outputs:
#         raise HTTPException(status_code=500, detail="No text generated by the model.")

#     completion_text = outputs[0].outputs[0].text
#     model_name = llm.llm_engine.model_config.model
#     return GenerateResponse(text=completion_text, model=model_name)


# @app.get("/")
# async def root() -> dict:
#     return {
#         "message": "vLLM inference server is running.",
#         "model": DEFAULT_MODEL,
#         "endpoints": {
#             "health": "/health",
#             "generate": "/generate",
#         },
#     }
# app.py



import uuid
from typing import AsyncGenerator, Awaitable, Callable, List, Optional

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from vllm import AsyncEngineArgs, AsyncLLMEngine
from vllm.sampling_params import SamplingParams

from .chat_store import get_logger

app = FastAPI(title="Llama 3.1 8B Instruct API", version="0.1.0")

# ----- CONFIG -----
#MODEL_NAME = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # name of model that we are using, this expects a huggingface repo name
MODEL_NAME = "Qwen/Qwen3-4B"
TENSOR_PARALLEL_SIZE = 1  # adjust based on gpu memory 
gpu_memory_utilization = 0.9  

# You can pass extra engine args here (tensor-parallel-size, gpu-memory-utilization, etc.)
engine_args = AsyncEngineArgs(
    model=MODEL_NAME,
    trust_remote_code=True,
    tensor_parallel_size=TENSOR_PARALLEL_SIZE,
    gpu_memory_utilization=gpu_memory_utilization,
    max_model_len=8192,  # allow up to 8K token sequences
)
engine: AsyncLLMEngine = AsyncLLMEngine.from_engine_args(engine_args)
chat_logger = get_logger()


# ----- REQUEST / RESPONSE SCHEMAS -----
class ChatMessage(BaseModel):
    role: str  # "system", "user", "assistant"
    content: str


class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    stream: bool = True
    persona: Optional[str] = None


# ----- HELPER: CONVERT CHAT MESSAGES TO PROMPT -----
def messages_to_prompt(messages: List[ChatMessage]) -> str:
    """
    vLLM can do chat templates automatically if the model has a tokenizer
    with chat_template, but to stay explicit we can just join.
    If your model has chat_template, you can instead use:
        engine.tokenizer.apply_chat_template(...)
    """
    # If your model has a chat template, use that:
    # return engine.tokenizer.apply_chat_template(
    #     [m.model_dump() for m in messages],
    #     tokenize=False,
    #     add_generation_prompt=True,
    # )
    parts = []
    for m in messages:
        if m.role == "system":
            parts.append(f"<|system|>\n{m.content}\n")
        elif m.role == "user":
            parts.append(f"<|user|>\n{m.content}\n")
        elif m.role == "assistant":
            parts.append(f"<|assistant|>\n{m.content}\n")
    parts.append("<|assistant|>\n")
    return "".join(parts)


# ----- STREAMING GENERATOR -----
async def stream_completion(
    prompt: str,
    sampling_params: SamplingParams,
    *,
    request_id: str,
    on_complete: Optional[Callable[[str], Awaitable[None]]] = None,
) -> AsyncGenerator[bytes, None]:
    """
    Streams chunks as simple JSON lines (you can adapt to SSE easily).
    Each yield is a line of JSON containing the new text delta.
    """
    results_generator = engine.generate(prompt, sampling_params, request_id=request_id)

    accumulated = ""
    try:
        async for request_output in results_generator:
            output = request_output.outputs[0]
            full_text = output.text[len(prompt):]
            delta = full_text[len(accumulated):] if full_text.startswith(accumulated) else full_text
            accumulated = full_text
            if delta:
                chunk = {
                    "id": request_id,
                    "event": "token",
                    "text": delta,
                    "finished": request_output.finished,
                }
                yield (f"{chunk}\n").encode("utf-8")

        done_chunk = {"id": request_id, "event": "end", "finished": True}
        yield (f"{done_chunk}\n").encode("utf-8")
    finally:
        if on_complete:
            try:
                await on_complete(accumulated)
            except Exception:
                # Logging must not break the response stream.
                pass


# ----- ROUTES -----
@app.post("/generate")
@app.post("/api/generate")
async def generate(req: ChatRequest):
    if not req.messages:
        raise HTTPException(status_code=400, detail="Messages list must not be empty.")

    conversation_payload = [msg.model_dump() for msg in req.messages]
    user_message = next(
        (msg for msg in reversed(conversation_payload) if msg.get("role") == "user"),
        None,
    )
    if user_message is None:
        raise HTTPException(
            status_code=400, detail="A user message is required to generate a response."
        )

    prompt = messages_to_prompt(req.messages)

    sampling_params = SamplingParams(
        max_tokens=req.max_tokens,
        temperature=req.temperature,
        top_p=req.top_p,
        stop=["</s>"],
    )
    metadata = {
        "max_tokens": req.max_tokens,
        "temperature": req.temperature,
        "top_p": req.top_p,
        "stream": req.stream,
    }
    request_id = str(uuid.uuid4())

    if req.stream:
        async def finalize(response_text: str) -> None:
            assistant_message = {"role": "assistant", "content": response_text}
            try:
                await chat_logger.log(
                    request_id=request_id,
                    persona=req.persona,
                    user_message=user_message,
                    assistant_message=assistant_message,
                    conversation=conversation_payload + [assistant_message],
                    metadata=metadata,
                )
            except Exception:
                # Logging failures should not break the API response path.
                pass

        return StreamingResponse(
            stream_completion(
                prompt,
                sampling_params,
                request_id=request_id,
                on_complete=finalize,
            ),
            media_type="text/plain",
        )
    else:
        results = await engine.generate(prompt, sampling_params, request_id=request_id)
        if not results:
            raise HTTPException(status_code=500, detail="No response from model")
        raw_output = results[0].outputs[0].text
        completion = raw_output[len(prompt):] if raw_output.startswith(prompt) else raw_output
        assistant_message = {"role": "assistant", "content": completion}
        try:
            await chat_logger.log(
                request_id=request_id,
                persona=req.persona,
                user_message=user_message,
                assistant_message=assistant_message,
                conversation=conversation_payload + [assistant_message],
                metadata=metadata,
            )
        except Exception:
            pass
        return {"id": request_id, "output": completion}


@app.get("/health")
@app.get("/api/health")
async def health() -> dict:
    """Basic readiness probe for the frontend."""
    return {"status": "ok", "model": MODEL_NAME}
